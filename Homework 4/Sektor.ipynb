{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.ml.linalg import Vectors\n",
    "import random\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "config = SparkConf().setAppName('Homework 3').setMaster('local')\n",
    "\n",
    "sc = SparkContext(conf=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful functions from the past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#K-Center fast implementation with Fartherst-Traversal First\n",
    "def kcenter(P, k):\n",
    "    #P_minus_S = [p for p in P]\n",
    "    idx_rnd = random.randint(0, len(P)-1)\n",
    "    S = [P[idx_rnd]]\n",
    "    dist_near_center = [Vectors.squared_distance(P[i], S[0]) for i in range(len(P))]\n",
    "\n",
    "    for i in range(k-1):    \n",
    "        new_center_idx = max(enumerate(dist_near_center), key=lambda x: x[1])[0] # argmax operation\n",
    "        S.append(P[new_center_idx])\n",
    "\n",
    "        for j in range(len(P)):\n",
    "            if j != new_center_idx:\n",
    "                dist = Vectors.squared_distance(P[j], S[-1])\n",
    "                if dist < dist_near_center[j]:\n",
    "                    dist_near_center[j] = dist\n",
    "            else:\n",
    "                dist_near_center[j] = 0\n",
    "    return S\n",
    "\n",
    "\n",
    "#Import toy dataset RDD\n",
    "\n",
    "def readVectorsSeq(filename):\n",
    "    file = open(filename, 'r')\n",
    "    vector_list = []\n",
    "    for row in file.readlines():\n",
    "        vector_list.append(Vectors.dense([float(num_str) for num_str in row.split()]))\n",
    "    return vector_list\n",
    "\n",
    "# Import the Dataset\n",
    "vec_list = readVectorsSeq('test-datasets/prova.txt')\n",
    "\n",
    "# Create a parallel collection\n",
    "dNumbers = sc.parallelize(vec_list)\n",
    "\n",
    "\n",
    "\n",
    "def runSequential(points, k):\n",
    "\n",
    "    n = len(points)\n",
    "    if k >= n:\n",
    "        return points\n",
    "\n",
    "    result = list()\n",
    "    candidates = np.full(n , True)\n",
    "\n",
    "    for iter in range(int(k / 2)):\n",
    "        maxDist = 0.0\n",
    "        maxI = 0\n",
    "        maxJ = 0\n",
    "        for i in range(n):\n",
    "            if candidates[i] == True:\n",
    "                for j in range(n):\n",
    "                    d = Vectors.squared_distance(points[i], points[j])\n",
    "                    if d > maxDist:\n",
    "                        maxDist = d\n",
    "                        maxI = i\n",
    "                        maxJ = j\n",
    "        result.append( points[maxI] )\n",
    "        result.append( points[maxJ] )\n",
    "        #print \"selecting \"+str(maxI)+\" and \"+str(maxJ)\n",
    "        candidates[maxI] = False\n",
    "        candidates[maxJ] = False\n",
    "\n",
    "    if k % 2 != 0:\n",
    "        s = np.random.randint(n)\n",
    "        for i in range(n):\n",
    "            j = (i + s) % n\n",
    "            if candidates[j] == True:\n",
    "                #print \"selecting \"+str(j)\n",
    "                result.append( points[i] )\n",
    "                break\n",
    "\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the Dataset and Define the Variables\n",
    "datafile = 'test-datasets/prova.txt'\n",
    "\n",
    "inputrdd = sc.textFile(datafile)\\\n",
    "                        .map(lambda row : Vectors.dense([float(num_str) for num_str in row.split(' ')]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this homework you need to develop the following two functions.\n",
    "\n",
    "A function runMapReduce(pointsrdd,k,numBlocks) that receives in input a set of points represented by an RDD pointsrdd of Vector and two integers k and numBlocks, and does the following things: \n",
    "\n",
    "       (a) partitions pointsrdd into numBlocks subsets; \n",
    "   \n",
    "       (b) extracts k points from each subset by running the sequential Farthest-First Traversal algorithm implemented for Homework 3; \n",
    "   \n",
    "       (c) gathers the numBlocks*k points extracted into a list of Vector coreset; \n",
    "   \n",
    "       (d) returns a a list of Vector with the k points determined by running the sequential max-diversity algorithm with input coreset and k. The code of the sequential algorithm can be downloaded here.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getMatrix(x):\n",
    "    mat = []\n",
    "    for element in x:\n",
    "        mat.append(element)\n",
    "    return mat\n",
    "\n",
    "def runMapReduce(pointsrdd, k, numBlocks):\n",
    "    #partitions pointsrdd into numBlocks subsets\n",
    "    distRDD = inputrdd.map(lambda x: (np.random.randint(numBlocks),x)).groupByKey() #map each vector with a random index. Given the uniform distribution, with a big dataset I expect the partition to be uniform.\n",
    "    \n",
    "    ##k-point extraction with Fartherst-First Traversal \n",
    "    coreset = distRDD.mapValues(lambda x: getMatrix(x)).mapValues(lambda x: 'AAAAAAAAAAA').collect()    \n",
    "    \n",
    "    # Extract k-Points maximal diversity on a single reducer\n",
    "    #list_max_diversity_output = runSequential(coreset,k)\n",
    "    \n",
    "    return coreset\n",
    "\n",
    "output = runMapReduce(inputrdd, 3, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'AAAAAAAAAAA'), (0, 'AAAAAAAAAAA'), (2, 'AAAAAAAAAAA')]\n"
     ]
    }
   ],
   "source": [
    "output = runMapReduce(inputrdd, 2, 3)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " A function measure(pointslist) that receives in input a set of points represented as a list pointslist and returns a double which is the average distance between all points in pointslist (i.e., the sum of all pairwise distances divided by the number of distinct pairs). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def measure(pointlist):\n",
    "    cumsum = 0\n",
    "    i=0\n",
    "    for point_one in pointlist:\n",
    "        for point_two in pointlist:\n",
    "            if point_one != point_two:\n",
    "                cumsum += np.linalg.norm(point_one-point_two, 2)\n",
    "                i = i+1\n",
    "    return cumsum/i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.361685673643496"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "measure(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1527518220.8152971"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<ipython-input-121-ad44d9e56c44>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-121-ad44d9e56c44>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    coreset = distRDD.mapValues(lambda x: getMatrix(x)).mapValues(lambda x: kcenter(x,k)).flatMap(lambda x: x[1]).collect()\u001b[0m\n\u001b[0m                                                                                                                               ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "def runMapReduce(pointsrdd, k, numBlocks):\n",
    "\tbefore = time.time()\n",
    "    #partitions pointsrdd into numBlocks subsets\n",
    "\tdistRDD = inputrdd.map(lambda x: (np.random.randint(numBlocks),x)).groupByKey() #map each vector with a random index. Given the uniform distribution, with a big dataset I expect the partition to be uniform.\n",
    "    \n",
    "    ##k-point extraction with Fartherst-First Traversal \n",
    "    coreset = distRDD.mapValues(lambda x: getMatrix(x)).mapValues(lambda x: kcenter(x,k)).flatMap(lambda x: x[1]).collect()    \n",
    "    print(\"Time taken for coreset construction: \" + str(time.time()-before))\n",
    "    # Extract k-Points maximal diversity on a single reducer\n",
    "\n",
    "    before = time.time()\n",
    "    list_max_diversity_output = runSequential(coreset,k)\n",
    "    print(\"Time taken for completing sequential algorithm: \" + str(time.time()-before))\n",
    "   \n",
    "    return list_max_diversity_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runMapReduce(pointsrdd, k, numBlocks):\n",
    "    #partitions pointsrdd into numBlocks subsets\n",
    "    before = time.time()\n",
    "    distRDD = inputrdd.map(lambda x: (np.random.randint(numBlocks),x)).groupByKey() #map each vector with a random index. Given the uniform distribution, with a big dataset I expect the partition to be uniform.\n",
    "    \n",
    "    ##k-point extraction with Fartherst-First Traversal \n",
    "    coreset = distRDD.mapValues(lambda x: getMatrix(x)).mapValues(lambda x: kcenter(x,k)).flatMap(lambda x: x[1]).collect()\n",
    "    print(\"Time taken for coreset construction: \" + str(time.time()-before))\n",
    "    \n",
    "    # Extract k-Points maximal diversity on a single reducer\n",
    "    before = time.time()\n",
    "    list_max_diversity_output = runSequential(coreset,k)\n",
    "    print(\"Time taken for completing sequential algorithm: \" + str(time.time()-before))\n",
    "\n",
    "    return list_max_diversity_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<ipython-input-124-545f2bfcc635>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-124-545f2bfcc635>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    distRDD = inputrdd.map(lambda x: (np.random.randint(numBlocks),x)).groupByKey() #map each vector with a random index. Given the uniform distribution, with a big dataset I expect the partition to be uniform.\u001b[0m\n\u001b[0m                                                                                                                                                                                                                  ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "def runMapReduce(pointsrdd, k, numBlocks):\n",
    "    \n",
    "\tbefore = time.time()\n",
    "    #partitions pointsrdd into numBlocks subsets\n",
    "    distRDD = inputrdd.map(lambda x: (np.random.randint(numBlocks),x)).groupByKey() #map each vector with a random index. Given the uniform distribution, with a big dataset I expect the partition to be uniform.\n",
    "    \n",
    "    ##k-point extraction with Fartherst-First Traversal \n",
    "    coreset = distRDD.mapValues(lambda x: getMatrix(x)).mapValues(lambda x: kcenter(x,k)).flatMap(lambda x: x[1]).collect()    \n",
    "    print(\"Time taken for coreset construction: \" + str(time.time()-before))\n",
    "    \n",
    "    # Extract k-Points maximal diversity on a single reducer\n",
    "    before = time.time()\n",
    "    list_max_diversity_output = runSequential(coreset,k)\n",
    "    print(\"Time taken for completing sequential algorithm: \" + str(time.time()-before))\n",
    "\n",
    "    return list_max_diversity_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
