Nella teoria dell'informazione l'entropia di una sorgente di messaggi è l'informazione media contenuta in ogni messaggio emesso. L'informazione contenuta in un messaggio è tanto più grande quanto meno probabile era. Un messaggio scontato, che ha un'alta probabilità di essere emesso dalla sorgente contiene poca informazione, mentre un messaggio inaspettato, poco probabile contiene una grande quantità di informazione. L'entropia di una sorgente risponde a domande come: qual è il numero minimo di bit che servono per memorizzare in media un messaggio della sorgente? Quanto sono prevedibili i messaggi emessi dalla sorgente? Si è dimostrato che una sequenza di messaggi emessi da una sorgente possono essere compressi senza perdita d'informazione fino ad un numero minimo di bit per messaggio uguale all'entropia della sorgente. Una sequenza di lettere come aaaaaaaa possiede meno entropia di una parola come alfabeto la quale a sua volta possiede ancora meno entropia di una stringa completamente casuale come j3s0vek3. L'entropia può essere vista come la casualità contenuta in una stringa, ed è strettamente collegata al numero minimo di bit necessari per rappresentarla senza errori.