{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Computing - Homework 1\n",
    "## Guglielmo Camporese, Ettore Mariotti, Matteo Drago, Riccardo Lincetto\n",
    "\n",
    "First thing to do, we need to import the two Spark modules from the **pyspark** library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we have to import the dataset from the text file on our current folder. To test if the import went well, we can print the list to the standard output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The locally loaded list of numbers is:  [1.0, 2.0, 3.0, 4.0]\n"
     ]
    }
   ],
   "source": [
    "# Import the Dataset\n",
    "lNumbers = []\n",
    "\n",
    "with open('dataset.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for n in lines:\n",
    "    \tif len(n[:-1]) > 0:\n",
    "    \t\tlNumbers.append(float(n[:-1]))\n",
    "\n",
    "print(\"The locally loaded list of numbers is: \", lNumbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to set up the Spark environment.\n",
    "\n",
    "As a simple setup we just set the name of the application that will appear on the cluster and we specify to run the code locally. The choice of local computation have been made for testing purposes.\n",
    "\n",
    "Then after having initialized a SparkContext object $sc$ with the specified configuration, we can build a RDD (resilient distribuited database) of $lNumbers$ by simply invoking the parallelize command on the sc object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Spark Setup\n",
    "conf = SparkConf().setAppName('Sum of Squares').setMaster('local')\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# Create a parallel collection\n",
    "dNumbers = sc.parallelize(lNumbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can evaluate the sum of sqaures using the MapReduce paradigm and lamba functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum of squares is 30.0\n"
     ]
    }
   ],
   "source": [
    "sumOfSquares = dNumbers.map(lambda s: s**2).reduce(lambda a, b: a + b)\n",
    "\n",
    "print(\"The sum of squares is \" + str(sumOfSquares))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
